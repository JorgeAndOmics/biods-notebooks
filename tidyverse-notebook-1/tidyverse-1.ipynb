{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Tidyverse: readr, dplyr, tidyr, purrr**\n",
    "###### **Ní Leathlobhair lab, Trinity College Dublin 2025**\n",
    "\n",
    "###### *Designed by Jorge G. García, 2025*\n",
    "---\n",
    "Welcome to this short introduction to tabular data manipulation with the **[Tidyverse](https://www.tidyverse.org/)** in **R**. While R remains one of the most powerful and versatile tools for statistical computing, it has often been criticized for its inconsistent syntax and steep learning curve. **Tidyverse** comes to the rescue, addressing these challenges through a coherent suite of packages that promote **readable**, **consistent**, and **expressive** code for data manipulation, visualization, and analysis.\n",
    "\n",
    "But more than just a collection of tools, the **Tidyverse** embodies a philosophy of data science. It encourages working with data in a **tidy**, **rectangular** form where each **variable is a column**, each **observation a row**, and each **type of observational unit a table**. This consistent data structure, paired with human-readable verbs like [`filter()`](https://dplyr.tidyverse.org/reference/filter.html), [`mutate()`](https://dplyr.tidyverse.org/reference/mutate.html), and [`summarise()`](https://dplyr.tidyverse.org/reference/summarise.html), makes code both expressive and easy to reason about. The **Tidyverse** favors **declarative pipelines** over procedural loops, aiming to mirror how analysts think about data.\n",
    "\n",
    "As we begin this journey, we'll learn how to transform a chaotic, messy dataset—the norm rather than the exception in real-world data—into something structured and insightful. We'll do this programmatically, but in a way that's *intuitive*, *coherent*, and *fluent*, thanks to the **Tidyverse**’s design. At the core of this workflow is the powerful **pipe operator** ([`%>%`](https://magrittr.tidyverse.org/reference/pipe.html)), which lets us chain operations together in a readable, step-by-step flow. It's not just a convenience—it's the beating heart of the **Tidyverse** paradigm, enabling data transformations that feel natural and expressive.\n",
    "\n",
    "This workshop is designed to equip you with tools and techniques that feel *intuitive* and *natural* to use—tools that might even change how you see R. I’ll admit, I was once firmly in the **R-skeptic** camp, but this approach transformed my experience: its **clarity**, **consistency**, and **expressive syntax** made R not just usable, but genuinely enjoyable—and my go-to language for anything involving **tabular data**. I haven’t looked back since.\n",
    "\n",
    "Even though we will use **real-world data** throughout most of the exercise, it is directed towards a wide audience and doesn't require any previous knowledge of **bioinformatic pipelines**, or even **R**! If you have no experience with code whatsoever, *fret not*! This exercise is designed to lead you effortlessly straight to the end. If for some reason you break it, restart and *voilà*. If you are already familiar with programming, this exercise should be of use to understand how **tabular data**, whatever its origin, can be manipulated in the **Tidyverse way**. Feel free to tinker with other parameters and explore how the code works!\n",
    "\n",
    "Last but not least, across the exercise you will see many **hyperlinks** scattered across the sections. Click on them to access **extra resources** that will enrich your experience, add context or reveal interesting facts about the lesson.\n",
    "\n",
    "With all this in mind, take a deep breath and press the **Run** button in the first cell. Welcome to the **Tidyverse**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suppressMessages({\n",
    "    library(arrow)\n",
    "    library(readr)\n",
    "    library(dplyr)\n",
    "    library(tidyr)\n",
    "    library(purrr)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congratulations!** You just imported the necessary libraries to perform all the analysis in this exercise. \n",
    "\n",
    "In programming, a **library** is a collection of precompiled routines—sets of instructions—that a program can use. These routines are designed to accomplish specific tasks, such as **handling files and data**, **performing mathematical computations**, or **managing network connections**—and that is exactly what we are going to do with these. Many of the libraries we will be using, like [`dplyr`](https://dplyr.tidyverse.org/) for **tabular data manipulation** or the powerful [`ggplot2`](https://ggplot2.tidyverse.org/) (next tutorial) for **plot generation**, are used everywhere across the world in many different fields, from **Google Analytics** to **epidemiology**.\n",
    "\n",
    "But where is the **Tidyverse**? Well, you see, these are the core components of the Tidyverse:\n",
    "\n",
    "| Package       | Description                                             |\n",
    "|---------------|---------------------------------------------------------|\n",
    "| [`ggplot2`](https://ggplot2.tidyverse.org/)   | Grammar of graphics for data visualization        |\n",
    "| [`dplyr`](https://dplyr.tidyverse.org/)       | Data manipulation (`filter`, `mutate`, `summarise`) |\n",
    "| [`tidyr`](https://tidyr.tidyverse.org/)       | Reshaping and tidying data                        |\n",
    "| [`readr`](https://readr.tidyverse.org/)       | Fast and consistent file I/O                      |\n",
    "| [`tibble`](https://tibble.tidyverse.org/)     | Modern reimagining of the dataframe               |\n",
    "| [`purrr`](https://purrr.tidyverse.org/)       | Functional programming with lists and vectors     |\n",
    "| [`stringr`](https://stringr.tidyverse.org/)   | Consistent, easy string manipulation              |\n",
    "| [`forcats`](https://forcats.tidyverse.org/)   | Tools for working with categorical variables      |\n",
    "| [`lubridate`](https://lubridate.tidyverse.org/)| Tools for date-time data handling                |\n",
    "\n",
    "But we could very much just do:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suppressMessages({\n",
    "    library(tidyverse)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it would work just as well. So why don’t we always do it that way?\n",
    "\n",
    "Well—if you need to hammer a nail into a wall, do you bring the entire toolbox, or just the hammer?\n",
    "\n",
    "The **Tidyverse** is a collection of packages designed to work together seamlessly, but that doesn’t mean you need to load all of them every time. In practice, it's often better to load only the packages you need for the task at hand. This keeps your code **cleaner**, **leaner**, and **easier to reason about**—especially in **collaborative** or **production environments**.\n",
    "\n",
    "That said, these libraries—when combined—cover the overwhelming majority of **tabular data manipulation tasks** a data scientist is likely to encounter. Their shared design principles mean that you can confidently **mix and match** them, chaining operations across packages without breaking mental flow. It’s not about loading everything—it’s about having the **right tool**, when you need it, ready to fit naturally into the pipeline.\n",
    "\n",
    "The **Tidyverse** is the result of a broader movement within the R community led by [Hadley Wickham](https://hadley.nz/), a statistician and software engineer who recognized the need for **consistency**, **expressiveness**, and **usability** in data science workflows within R. Prior to the **Tidyverse**, R was powerful but incoherent:\n",
    "\n",
    "\n",
    "- Functions for similar tasks had wildly inconsistent interfaces.\n",
    "\n",
    "- Base R’s syntax was often cryptic or verbose.\n",
    "\n",
    "- There were multiple ways to do everything, but no clear best practices.\n",
    "\n",
    "- Data cleaning and manipulation—often the bulk of data work—was painful.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "We are ready to start our analysis. What will we be analysing? This is your story: You just got accepted in the prestigious **[Trinity College Dublin](https://www.tcd.ie/)**, and you are eager to start your new life as a student. But your first challenge lies outside the campus walls. First stop, the **Irish accommodation market**. You need to find a place to live, and you want to make sure you get the **best deal possible**.\n",
    "\n",
    "\n",
    "[^1]: test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data <- readr::read_csv('irish_accomodations_augmented.csv', show_col_types = FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far so good, right?\n",
    "\n",
    "Well, kind of.\n",
    "\n",
    "Well, actually, no.\n",
    "\n",
    "Let's take a look at some qualities of our loaded dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the onset, we can see many things that would set any data scientist worth their salt into alert:\n",
    "\n",
    "- Multiple types of data: **Strings**, **decimals** (*floats* or *doubles*), **integers**, **lists**.\n",
    "- Liberal use of **upper case**, both in features (column names) and in the data.\n",
    "- `NA` (*Null*) values present in several rows.  \n",
    "\n",
    "Let’s go with the first (and most urgent) one.  \n",
    "The interpreter has done an admirable job at categorizing whatever data columns it found in categories (`<dbl>`, `<chr>`),  \n",
    "but many of them are either unnecessary or outright wrong.\n",
    "\n",
    "So, where’s the problem?\n",
    "\n",
    "It’s the **CSV file**.  \n",
    "It’s *always* been the CSV file.\n",
    "\n",
    "---\n",
    "\n",
    "**CSV files** are the data scientist’s perennial source of headaches.  \n",
    "They are a **messy way of storing complex datasets** for mainly (but not only) the following reasons:\n",
    "\n",
    "- **Everything** in them is stored as **plain text**.  \n",
    "  That means **hidden unicode characters**, **linebreaks**, **invisible tab spaces** — the list goes on and on.\n",
    "\n",
    "- **Everything** in them is stored as **plain text**.  \n",
    "  That means **no compression** whatsoever. Every single character is a Unicode symbol, with a corresponding **load on your working memory**.\n",
    "\n",
    "- **Everything** in them is stored as **plain text**.  \n",
    "  That means **no data types**.  \n",
    "  There are no **numbers**, no **integers**, no **factors**, no **lists**.  \n",
    "  Everything in them is a **character**.  \n",
    "  You wanted to keep those **three decimals** in the second column?  \n",
    "  Too bad! Depending on the next interpreter, it may decide **two are enough**.\n",
    "\n",
    "- **Everything** in them is stored as **plain text**.  \n",
    "  **No traceability** whatsoever.  \n",
    "  It’s just impossible to retroengineer what type of data each feature contained when this dataset was turned into a **text file**.  \n",
    "  We are off to a bad start as **reproducibility** goes!\n",
    "\n",
    "---\n",
    "\n",
    "You are a perceptive fellow (you’ve been accepted at [TCD](https://www.tcd.ie/), after all), so you start to pinpoint where the trouble lies.\n",
    "\n",
    "Let’s make it clear one last time:\n",
    "\n",
    "### **Plain text bad**\n",
    "\n",
    "---\n",
    "\n",
    "Okay, but you must be wondering:\n",
    "\n",
    "> *\"It can’t be that bad, right? It’s the format I’ve used my whole life—and the one every course, teacher, and tutorial insisted on.\"*\n",
    "\n",
    "Well, you are **partially right**.\n",
    "\n",
    "Perhaps data types in your particular dataset are not very difficult to infer or reconstruct.  \n",
    "Perhaps most people simply don’t care if an invisible space gets in the way or the file is 2MB heavier.\n",
    "\n",
    "But we are **data scientists**.  \n",
    "Our data might be messy (it usually is), but we are **precise**.  \n",
    "If we say a column contains **three decimals**, there **must** be three decimals.\n",
    "\n",
    "---\n",
    "\n",
    "Let’s try something else:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data <- arrow::read_parquet('irish-accomodations-augmented.parquet')\n",
    "\n",
    "head(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice anything different? Here’s what’s changed:\n",
    "\n",
    "- We now have **many more data types**: `<int>`, `<dbl>`, `<chr>`, `<fct>`.  \n",
    "  We even have some odd ones like `<list<dbl>>`. Would you have noticed without additional info?\n",
    "\n",
    "- Our dataset is now **_lazy-loaded_**.  \n",
    "  What does that mean?  \n",
    "  Your environment **only loads the parts of the dataset you're actually using — nothing more**.  \n",
    "  If you were loading a `.csv`, you'd be pulling the **entire** file into memory, slowing down every step of your analysis.\n",
    "\n",
    "- The generated object is a **`tibble`**, not a base `data.frame`.  \n",
    "  A tibble is a **modern, strict, and tidy-aware** form of tabular data that’s easier to read and work with.\n",
    "\n",
    "- The file size is **~3× smaller** than its `.csv` equivalent.\n",
    "\n",
    "- And while it might not be obvious at this scale, the file has been loaded **much** faster than the `.csv` version.\n",
    "\n",
    "---\n",
    "\n",
    "Now imagine you're working with a massive **genomic dataset**: Tens of millions of rows, hundreds of columns.  \n",
    "Chances are your laptop can’t even load the full CSV into memory — and if it does, you’ll lose metadata and pay the price in speed.\n",
    "\n",
    "This is where **Parquet** comes in. Enter: `arrow` and the Parquet format.\n",
    "\n",
    "**Officially endorsed by Hadley Wickham** (chief tidyverse author), the [`arrow`](https://arrow.apache.org/docs/r/) library is the **go-to** for working with `.parquet` files in R.  \n",
    "These files belong to a family of **sequential binary formats** — not human-readable, but:\n",
    "\n",
    "- **Highly compressed**\n",
    "- **Efficiently queryable**\n",
    "- **Rich in metadata**\n",
    "- **Built for scale**\n",
    "\n",
    "Something as simple as switching to Parquet can **speed up and lighten your workflows by orders of magnitude**, especially as data generation accelerates in the coming years.\n",
    "\n",
    "---\n",
    "\n",
    "But enough with lazy loadings. Let's look at the data once more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first stop is the incredibly powerful [`dplyr`](https://dplyr.tidyverse.org/) (pronounced “**dee-ply-er**”) library from the **Tidyverse**.  \n",
    "`dplyr` contains several commands and options for **tabular data manipulation**.\n",
    "\n",
    "For example, notice another flaw in our dataset: **The column names**.\n",
    "\n",
    "In computer science in general, **simple usually beats complex** nine out of ten times.  \n",
    "In feature and file naming, it's actually **ten out of ten** times. This is because:\n",
    "\n",
    "- **Longer names** → Higher chance of human error  \n",
    "- **More case types** → Higher chance of human error  \n",
    "- **Special characters** → Higher chance of human error  \n",
    "- **Spaces** → Higher chance of human error + Computer mishandling  \n",
    "- **More complex names** → Higher chance of human error + Variable confusion  \n",
    "\n",
    "There are few adagios more repeated in **bioinformatics**, or plain **data science** across time than this one:  \n",
    "**Clear, simple, informative names.**\n",
    "\n",
    "So we are changing the column names for something **clear**, **simple**, and **informative**.  \n",
    "Enter [`rename()`](https://dplyr.tidyverse.org/reference/rename.html):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_renamed <- dplyr::rename(data, name = Name)\n",
    "\n",
    "head(data_renamed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the straightforward syntax, almost bordering **natural language processing**.  \n",
    "We can concatenate as many columns as we want within a single command call using a `,`.  \n",
    "\n",
    "Let’s finish with the rest of the columns:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_renamed <- rename(data_renamed, \n",
    "                       url = Url,\n",
    "                       telephone = Telephone,\n",
    "                       longitude = Longitude,\n",
    "                       latitude = Latitude,\n",
    "                       region = AddressRegion,\n",
    "                       locality = AddressLocality,\n",
    "                       country = AddressCountry,\n",
    "                       tags = Tags,\n",
    "                       price = Price_EUR,\n",
    "                       tag_region = Tag_Region,\n",
    "                       rating = Rating,\n",
    "                       unit = Unit,\n",
    "                       section = Section,\n",
    "                       c_list = ComplexList)\n",
    "\n",
    "head(data_renamed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better. What about some light filtering?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filtered <- filter(data_renamed, price <= 800)\n",
    "\n",
    "head(data_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Notice how our column remains with the new name `price`. Always remember to store your modifications within a variable (`data_XXX <- operation(data)`)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`filter` allows us to select rows based on comparison operators. This command has selected all rows where the column `price` was equal or less than 800. We can specify more complex thresholds using logical operators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filtered <- filter(data_renamed,\n",
    "                        price >= 800 & price < 1000,   # AND operator\n",
    "                        latitude > 45 | longitude < -5,    # OR operator\n",
    "                        between(section, 10, 100))    # between function, defines a range\n",
    "\n",
    "head(data_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also perform categorical comparisons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filtered <- filter(data_filtered,\n",
    "                        tag_region == 'Camping_Donegal')    # Notice we are using \"==\", not \"=\"\n",
    "\n",
    "head(data_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aren't there too many columns? We could easily fix that with the powerful `select` function from `dplyr`. It allows to simply and intuitively pick the features that we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_selected <- dplyr::select(data_renamed,\n",
    "                               name,\n",
    "                               url,\n",
    "                               telephone,\n",
    "                               region,\n",
    "                               locality,\n",
    "                               price)\n",
    "\n",
    "head(data_selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But selecting one by one each of the columns we want to preserve can quickly become tedious; and worst of all, **error-prone**. `select` allows us to \"drop\" features from the dataset just by adding a `-` symbol before the undesired columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_selected <- dplyr::select(data_renamed,\n",
    "                               -latitude,\n",
    "                               -longitude,\n",
    "                               -country,\n",
    "                               -tags,\n",
    "                               -tag_region,\n",
    "                               -unit,\n",
    "                               -section,\n",
    "                               -c_list)\n",
    "\n",
    "head(data_selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And of course, you can seamlessly select and drop in the same function call.\n",
    "\n",
    "_1. You may have noticed that for some functions we use the prefix `dplyr::`, like in `dplyr::select`. This is to explicitly call the `select` function from `dplyr`. \"Select\", being a rather common word, is often used as a function name for other packages, generating \"namespace conflicts\". They are a silent but vicious source of bugs, trust me!_\n",
    "\n",
    "_2. You may end up having to \"deselect\" more features than if you went the other way around. Always strive to make your code both efficient and easy to read!_\n",
    "\n",
    "---\n",
    "\n",
    "We can also sort our data with the `arrange` function. `arrange` will sort the whole table in numerical order. For categorial variables, it will use alphabetical order. To sort our data by just one variable, we can do it like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_arranged <- arrange(data_renamed,\n",
    "                         rating)\n",
    "\n",
    "head(data_arranged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can sort them in descending order using the `desc()` accessory function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_arranged <- arrange(data_renamed,\n",
    "                         desc(rating))\n",
    "\n",
    "head(data_arranged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And of course, we can arrange the dataset by two or more features, in any order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_arranged <- arrange(data_renamed,\n",
    "                         desc(rating),\n",
    "                         price,\n",
    "                         locality)\n",
    "\n",
    "head(data_arranged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I’d like to introduce what is probably `dplyr`’s most used function: [`mutate()`](https://dplyr.tidyverse.org/reference/mutate.html).  \n",
    "`mutate()` generates **a whole new column** based on the data available in other specified columns.\n",
    "\n",
    "For example, let’s say we want to create a column with the price in **Chinese yuan (CNY)**.  \n",
    "Assuming the current exchange rate is **1 EUR = 8.29 CNY**, the operation would go like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mutated_yuan <- mutate(data_renamed,\n",
    "                            price_yuan = price * 8.29)\n",
    "\n",
    "head(data_mutated_yuan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can define a new, more complex feature.  \n",
    "For example, we can enrich our *rating* metric with a **normalized** version of the *price* metric.\n",
    "\n",
    "Let’s define the **minimum** and **maximum** values of the *price* column,  \n",
    "then use them to obtain a **rescaled** metric that we can add to our *rating* feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_min <- min(data_renamed$price)\n",
    "\n",
    "price_max <- max(data_renamed$price)\n",
    "\n",
    "data_mutated_rating <- mutate(data_mutated_yuan,\n",
    "                              rating_enriched = as.integer(rating) + ((price - price_min) / (price_max - price_min)))\n",
    "\n",
    "head(data_mutated_rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There it is! Although you may be missing it because it's at the very far end of the table. Fret not! `relocate` comes to the rescue!\n",
    "\n",
    "---\n",
    "\n",
    "_Yes, there is a function in `dplyr` to obtain minimum and maximum values of a vertor feature. We will cover it later in this tutorial._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_relocated <- relocate(data_mutated_rating,\n",
    "                           price_yuan,\n",
    "                           .after = price)\n",
    "\n",
    "data_relocated <- relocate(data_relocated,\n",
    "                           rating_enriched,\n",
    "                           .before = rating)\n",
    "\n",
    "head(data_relocated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, you can define the original column names, in which case the original column will be replaced with the new version. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_min <- min(data_renamed$price)\n",
    "\n",
    "price_max <- max(data_renamed$price)\n",
    "\n",
    "data_mutated_rating_original <- mutate(data_renamed,\n",
    "                                rating = as.integer(rating) + ((price - price_min) / (price_max - price_min)))\n",
    "\n",
    "head(data_mutated_rating_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Observe how the new rating type went from`<fct>` to `<dbl>`. This is the type of metadata that we want to preserve whenever possible!_\n",
    "\n",
    "There's a \"version\" of mutate, `transmute`, that performs the exact same operation. The only difference is that this one **returns only the modified column**. If we run the exact same command as before, but with `transmute` instead of `mutate`, this is what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_min <- min(data_renamed$price)\n",
    "\n",
    "price_max <- max(data_renamed$price)\n",
    "\n",
    "data_transmuted_rating_original <- transmute(data_renamed,\n",
    "                                rating = as.integer(rating) + ((price - price_min) / (price_max - price_min)))\n",
    "\n",
    "head(data_transmuted_rating_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neat, isn’t it? Well,\n",
    "\n",
    "### **Buckle up**\n",
    "\n",
    "This is the perfect moment to introduce the hero of our story: the pipe operator [`%>%`](https://magrittr.tidyverse.org/reference/pipe.html).\n",
    "\n",
    "This deceptively simple symbol is incredibly powerful. It allows us to **chain together any function** within the **entire Tidyverse ecosystem**,  \n",
    "acting as seamless glue between libraries and operations. It turns step-by-step logic into clean, readable pipelines.\n",
    "\n",
    "Let’s now perform **all** the previous operations in a **single command**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transformed <- data_renamed %>%\n",
    "                        filter(price >= 800 & price < 1000,\n",
    "                               latitude > 45 | longitude < -5,\n",
    "                               between(section, 10, 100)) %>%\n",
    "                        select(-country,\n",
    "                               -section) %>%\n",
    "                        arrange(desc(rating),\n",
    "                                price,\n",
    "                                locality) %>%\n",
    "                        mutate(price_yuan = price * 8.29,\n",
    "                               rating_enriched = as.integer(rating) + ((price - price_min) / (price_max - price_min))) %>%\n",
    "                        relocate(price_yuan, .after = price) %>%\n",
    "                        relocate(rating_enriched, .before = rating)\n",
    "\n",
    "head(data_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as long as the operations are **logically valid**, you can keep passing the dataset to other functions **indefinitely**.  \n",
    "\n",
    "In the following tutorials, we’ll explore how the **exact same syntax** allows us to pipe this tibble directly into Tidyverse’s powerful [`ggplot2`](https://ggplot2.tidyverse.org/) library to start creating plots immediately.  \n",
    "The same holds true for **any other library** within the broader **Tidyverse ecosystem**.\n",
    "\n",
    "Even more exciting: in future practicals, we’ll learn how to apply this same syntax to **genomic ranges** using **[Bioconductor](https://www.bioconductor.org/)** — so make sure to commit it to memory!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let’s turn our attention to two powerful `dplyr` functions that are sure to become some of your most trusted allies when working with all kinds of data: `group_by` and `summarize`.\n",
    "\n",
    "`group_by` is a somewhat unique function in the `dplyr` toolkit. On its own, it doesn’t return **any visible result**—instead, it silently creates groups within your dataset based on the features you select. These groups then allow subsequent operations to be performed **independently** on each one. It’s a bit tricky to describe, and its learning curve is slightly steeper than that of most other `dplyr` functions—definitely a challenge for the bravest of data-driven house hunters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_group <- data_transformed %>%\n",
    "                  group_by(region)\n",
    "\n",
    "head(data_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See? Nothing happened.\n",
    "\n",
    "The magic begins when we combine it with other functions—chief among them, [`summarise()`](https://dplyr.tidyverse.org/reference/summarise.html):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_group <- data_transformed %>%\n",
    "                  group_by(region) %>%\n",
    "                  summarize(min_price = min(price),\n",
    "                            max_price = max(price))\n",
    "\n",
    "data_group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returned tibble is **not** our original—and that’s perfectly fine.  \n",
    "\n",
    "We can use [`summarise()`](https://dplyr.tidyverse.org/reference/summarise.html) to compute **all kinds of statistics** on our groups.  \n",
    "Keep in mind: without [`group_by()`](https://dplyr.tidyverse.org/reference/group_by.html), these operations are performed on **the entire dataset**.\n",
    "\n",
    "Feel free to tinker with it and see what happens!\n",
    "\n",
    "_1. See? I told you we can calculate mins and maxes with `dplyr`._\n",
    "\n",
    "---\n",
    "\n",
    "Let’s take another look at our newly transformed dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(data_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’re ready to start contacting some of the available options, sorted by **price** and **rating**.  \n",
    "\n",
    "But wait—didn’t we have some **empty cells** in there?  \n",
    "Let’s take a look:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transformed %>% filter(is.na(url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See what we did there? Yes, you can use filter with **any** function, as long as it returns a boolean (True or False). Keep that in mind!\n",
    "\n",
    "_1. Also check how we didn't have to call our new data because we didn't store it as a variable._\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’re a thorough person—and you like your data thorough as well.  \n",
    "Those `NA` values are looking at you funny, and you don’t like that.\n",
    "\n",
    "In science, `NA` values range from **noisome inconveniences** to **Lovecraftian nightmares** that can ruin your entire analysis.  \n",
    "We don’t like them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full <- data_transformed %>%\n",
    "                tidyr::drop_na(url)\n",
    "\n",
    "data_full %>% filter(is.na(url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to your very first [`tidyr`](https://tidyr.tidyverse.org/) function.  \n",
    "`tidyr` is a close cousin of `dplyr`, and a powerful toolkit for **data rearrangement**.  \n",
    "\n",
    "Remember: full compatibility with `dplyr`—and almost any other Tidyverse function—is guaranteed by the [`%>%`](https://magrittr.tidyverse.org/reference/pipe.html) operator!\n",
    "\n",
    "In this case, our previous command to detect `NA` values in the *url* column returns an **empty tibble**.  \n",
    "[`drop_na()`](https://tidyr.tidyverse.org/reference/drop_na.html) took care of them!\n",
    "\n",
    "There are **many, many ways** to perform the same operation in R—but always remember the **first principle of engineering**:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\pi \\approx e \\approx 3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nope! Not that one. This one:\n",
    "\n",
    "### **Don’t reinvent the wheel.**\n",
    "\n",
    "If an open-source solution audited by millions of people every day **works**,  \n",
    "it’s very likely to cover **more edge cases** and have **more robust fallback mechanisms** than your well-intentioned custom function.  \n",
    "**Trust the Tidyverse.**\n",
    "\n",
    "---\n",
    "\n",
    "But data rarely comes with **clean column names**, the right number of `NA` values to make us feel good,  \n",
    "or **neatly defined categorical features**.  \n",
    "More often than not, we have to **create** or **reshape** them ourselves.\n",
    "\n",
    "Introducing `tidyr`’s [`unite()`](https://tidyr.tidyverse.org/reference/unite.html) and [`separate()`](https://tidyr.tidyverse.org/reference/separate.html):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_composite_location <- data_transformed %>%\n",
    "                               unite(comp_location, longitude, latitude, sep = \" & \", remove = FALSE)  # Observe the remove argument. What does it do?\n",
    "\n",
    "head(data_composite_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See what we got there? A new column, made from two. Very useful, although often not as much as its counterpart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_separate_modality <- data_transformed %>%\n",
    "                              separate(tag_region, into = c(\"mode\", \"region_2\"), sep = \"_\")\n",
    "\n",
    "head(data_separate_modality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we have two new columns from one. Observe that we defined _region_2_ because we already had a region column. Observe as well that we have to specify the new column names as strings (\"mode\", \"region\"). Try using the name _region_ as the second value name and see what happens!\n",
    "\n",
    "_1. You are a smart fellow, so you have noticed outright that we didn't have to `mutate` the tibble to integrate the new columns._\n",
    "\n",
    "---\n",
    "\n",
    "Last but not least, I’d like to introduce you to the [`purrr`](https://purrr.tidyverse.org/) package.\n",
    "\n",
    "R is a language that lends itself particularly well to the **functional programming paradigm**.  \n",
    "In this paradigm, you define **functions**, and then **map features** to those functions.  \n",
    "They offer an elegant alternative to classic loops, letting you remain in full control of the operation simply by inspecting the mapped function.\n",
    "\n",
    "As an alternative to the well-known `apply` family in base R (`lapply`, `sapply`, etc.),  \n",
    "`purrr` provides:\n",
    "\n",
    "- **Safer execution**\n",
    "- **Predictable return types**\n",
    "- **Seamless integration** with other Tidyverse tools\n",
    "- A **clean, readable syntax**\n",
    "\n",
    "We’ll use it alongside our favorite function, [`mutate()`](https://dplyr.tidyverse.org/reference/mutate.html):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_purrred <- data_transformed %>%\n",
    "                    mutate(c_list_cv = map_dbl(c_list, ~ sd(.) / mean(.)))\n",
    "\n",
    "head(data_purrred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, what just happened. What’s that `~`? Why is there a dot out of nowhere? What’s even a *c_list_cv*?  \n",
    "Let’s break it down:\n",
    "\n",
    "- `~` is used in R… for a lot of things. But one of its most powerful use cases is for **anonymous functions**.  \n",
    "  These are function definitions written on the fly—identical in purpose to [*lambda functions*](https://python-tutorials.in/python-lambda-functions-with-examples/) in Python.  \n",
    "  They let us define a \"single-use\" function directly in place.  \n",
    "  It’s a compact and expressive way to write logic without cluttering the code with one-off named functions.  \n",
    "  Still, when in doubt, always favor clarity: **write the full function explicitly** if it improves legibility.\n",
    "\n",
    "- `.` refers to the **current element** being processed.  \n",
    "  In this case, each element in the `c_list` column is a `<list<dbl>>`.  \n",
    "  As [`map()`](https://purrr.tidyverse.org/reference/map.html) applies the function, `.` stands in for each of these lists.  \n",
    "  The function then runs **vectorized operations** on them one-by-one. *(See why data types are a big deal yet?)*  \n",
    "  Could you pull this off using just [`mutate()`](https://dplyr.tidyverse.org/reference/mutate.html)? Try it and find out—**exploration is part of the journey**.  \n",
    "  And remember: every Tidyverse tool can be chained with `%>%`!\n",
    "\n",
    "- What’s CV? The **coefficient of variation**, and here’s the formula:  \n",
    "  $\\text{CV} = \\frac{\\sigma}{\\mu}$\n",
    "\n",
    "- What’s `c_list`?  \n",
    "  *I have no idea either!*\n",
    "\n",
    "---\n",
    "\n",
    "With your newly polished dataset in hand, you’re more prepared than ever to tackle your housing hunt.  \n",
    "**Good luck out there—we’ll meet again soon to learn how to visualize your findings!**\n",
    "\n",
    "---\n",
    "\n",
    "This concludes our brief introduction to the **Tidyverse** and some of its most essential functions.  \n",
    "The good news? We’ve only scratched the surface.  \n",
    "There’s a vast landscape of tools waiting to be explored—entire libraries packed with functionality.  \n",
    "(If you’re curious, [`stringr`](https://stringr.tidyverse.org/) is a great next stop, especially for mastering **regular expressions** and **string manipulation**.)\n",
    "\n",
    "Many of the functions we’ve just encountered have **alternate forms**, **variants**, and **deep parameter sets** that let you refine behavior with surgical precision.  \n",
    "I wasn’t exaggerating: the Tidyverse really does contain **most of the tools a data scientist needs** in their day-to-day work.\n",
    "\n",
    "The bad news?  \n",
    "We’ve only scratched the surface.  \n",
    "\n",
    "Like with any form of coding, true fluency comes not from reading, but from **doing**—your fingers will learn before your brain does.  \n",
    "And with great power comes great… debugging.  \n",
    "Some errors will silently haunt your code, leaving you bewildered for days, only to be traced back to a **missing comma**.  \n",
    "You’ll be **awed** by what the Tidyverse can do, and occasionally **infuriated** by its more cryptic behavior.  \n",
    "**Be patient.** This toolkit powers insights at some of the world’s most advanced organizations.  \n",
    "If it doesn’t work the way you want, don’t blame the hammer—**learn how to swing it better**.\n",
    "\n",
    "As you grow more comfortable, you’ll realize the **Tidyverse isn’t just a collection of packages—it’s a philosophy**.  \n",
    "You’ll stop thinking in terms of functions and start thinking in terms of **transformations**.  \n",
    "You’ll begin to **intuit how tools can chain together**, how **pipelines** can flow, and how operations can be reduced to their most **expressive form**.  \n",
    "Eventually, you’ll reach the point where you use R not just as a programming language,  \n",
    "but as a **language for thinking statistically**.  \n",
    "And that’s exactly what it was built for.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’ve now taken your first confident steps into the **Tidyverse**—but this is just the beginning.  \n",
    "The ecosystem is vast, the tools are deep, and your journey toward fluent, expressive data science in R has (perhaps) only just started.\n",
    "\n",
    "Below is a curated set of **hand-picked resources** to help you go further, deeper, and faster.\n",
    "\n",
    "---\n",
    "\n",
    "### 📚 **Official Documentation & Cheat Sheets**\n",
    "\n",
    "- **Tidyverse Main Site**: [https://www.tidyverse.org](https://www.tidyverse.org)  \n",
    "  Gateway to all packages, updates, and community links.\n",
    "\n",
    "- **Tidyverse Cheat Sheets (PDF)**:  \n",
    "  The fastest way to internalize syntax and workflows.  \n",
    "  → [https://posit.co/resources/cheatsheets/](https://posit.co/resources/cheatsheets/)\n",
    "\n",
    "- **R for Data Science** (2nd edition) by Hadley Wickham & Mine Çetinkaya-Rundel  \n",
    "  The canonical book—free online!  \n",
    "  → [https://r4ds.hadley.nz/](https://r4ds.hadley.nz/)\n",
    "\n",
    "---\n",
    "\n",
    "### 🧬 **Bioinformatics Extensions**\n",
    "\n",
    "If you´ve come for the transcriptomics, you'll want to check out:\n",
    "\n",
    "- **Bioconductor**:  \n",
    "  The de facto standard for R-based bioinformatics workflows  \n",
    "  → [https://www.bioconductor.org](https://www.bioconductor.org)\n",
    "\n",
    "- **`tidybulk`**: Tidyverse-style RNA-seq analysis  \n",
    "  → [https://stemangiola.github.io/tidybulk/](https://stemangiola.github.io/tidybulk/)\n",
    "\n",
    "- **plyranges**: Tidyverse-style genomic range manipulation  \n",
    "  → [https://www.bioconductor.org/packages/release/bioc/html/plyranges.html](https://www.bioconductor.org/packages/release/bioc/html/plyranges.html)\n",
    "\n",
    "---\n",
    "\n",
    "### 🎓 **Learning by Doing**\n",
    "\n",
    "- **Posit Cloud (formerly RStudio Cloud)**:  \n",
    "  Write and run R code in the cloud, no setup required  \n",
    "  → [https://posit.cloud](https://posit.cloud)\n",
    "\n",
    "- **TidyTuesday**:  \n",
    "  Weekly datasets + community visualizations  \n",
    "  → [https://github.com/rfordatascience/tidytuesday](https://github.com/rfordatascience/tidytuesday)\n",
    "\n",
    "- **Swirl**: Learn R inside R, interactively  \n",
    "  → [https://swirlstats.com](https://swirlstats.com)\n",
    "\n",
    "- **Kaggle**: Challenges and datasets (like the one you just used)  \n",
    "  → [https://www.kaggle.com/](https://www.kaggle.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
